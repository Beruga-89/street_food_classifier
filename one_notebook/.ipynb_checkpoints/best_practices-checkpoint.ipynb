{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c8de54f-e59d-4e83-95ec-6c905a3cc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Union, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Data analysis & visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "\n",
    "# ML/DL Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Other\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523d54b-c4b4-41d7-901d-9549d3b55492",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. CONFIGURATION CLASS \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20fddaf9-97fb-41a8-9362-57766cd27684",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Zentrale Konfigurationsklasse für alle Hyperparameter und Pfade.\"\"\"\n",
    "    \n",
    "    # Training Hyperparameter\n",
    "    BATCH_SIZE: int = 16\n",
    "    LEARNING_RATE: float = 1e-4\n",
    "    EPOCHS: int = 10\n",
    "    IMG_SIZE: int = 224\n",
    "    SEED: int = 42\n",
    "    \n",
    "    # Pfade\n",
    "    DATA_FOLDER: Path = Path('popular_street_foods/dataset')\n",
    "    MODEL_FOLDER: str = \"models\"\n",
    "    SAMPLE_FOLDER: str = \"sample\"\n",
    "    \n",
    "    # Dateinamen\n",
    "    PLOT_IMAGE_PATH: str = 'plot_image.png'\n",
    "    HISTORY_PATH: str = \"history.json\"\n",
    "    BEST_F1_MODEL_PATH: str = 'best_f1_model.pth'\n",
    "    BEST_ACC_MODEL_PATH: str = 'best_acc_model.pth'\n",
    "    BEST_LOSS_MODEL_PATH: str = 'best_loss_model.pth'\n",
    "    \n",
    "    # Training Parameter\n",
    "    PATIENCE: int = 5\n",
    "    GAMMA: float = 0.1\n",
    "    \n",
    "    # Prediction\n",
    "    PREDICTION_THRESHOLD: float = 0.4\n",
    "    UNKNOWN_LABEL: str = \"unknown\"\n",
    "    \n",
    "    # Data Split\n",
    "    TRAIN_SPLIT: float = 0.8\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Erstelle Ordner falls sie nicht existieren.\"\"\"\n",
    "        os.makedirs(self.MODEL_FOLDER, exist_ok=True)\n",
    "        os.makedirs(self.SAMPLE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6711fa-8551-434c-a53a-19fd6b3a4b6f",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. LOGGER SETUP\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56584d1-9164-4885-8fc0-6ab7f5cba3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name: str = __name__, level: int = logging.INFO) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Erstellt einen strukturierten Logger für das Projekt.\n",
    "    Windows-kompatibel ohne Unicode-Probleme.\n",
    "    \n",
    "    Args:\n",
    "        name: Name des Loggers\n",
    "        level: Logging Level\n",
    "        \n",
    "    Returns:\n",
    "        Konfigurierter Logger\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Verhindere doppelte Handler\n",
    "    if not logger.handlers:\n",
    "        # File Handler mit UTF-8 Encoding für Windows\n",
    "        file_handler = logging.FileHandler('training.log', encoding='utf-8')\n",
    "        file_handler.setLevel(level)\n",
    "        \n",
    "        # Console Handler - nur für wichtige Meldungen\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.WARNING)  # Weniger Console-Output\n",
    "        \n",
    "        # Formatter ohne Emojis für Windows-Kompatibilität\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        \n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54837e98-3131-4f5d-ae3c-64e8c8537292",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. UTILITY FUNCTIONS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e05668-417a-451e-8339-ca8bfefb4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Setzt alle Random Seeds für Reproduzierbarkeit.\n",
    "    \n",
    "    Args:\n",
    "        seed: Random Seed Wert\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Bestimmt das beste verfügbare Device.\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: CUDA falls verfügbar, sonst CPU\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53dcc7-ad67-4c1a-850f-bd5026f9e83b",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. DATA HANDLING CLASS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9790c8-552d-41fe-8633-2aa101dfad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"Verwaltet alle Datenoperationen.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Validierung dass config korrekt ist\n",
    "        if not hasattr(config, 'DATA_FOLDER'):\n",
    "            raise AttributeError(\"Config must have DATA_FOLDER attribute\")\n",
    "        if not hasattr(config, 'BATCH_SIZE'):\n",
    "            raise AttributeError(\"Config must have BATCH_SIZE attribute\")\n",
    "            \n",
    "    def get_transforms(self) -> Tuple[T.Compose, T.Compose]:\n",
    "        \"\"\"\n",
    "        Erstellt Transformationen für Training und Validierung.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple mit (train_transform, val_transform)\n",
    "        \"\"\"\n",
    "        train_transform = T.Compose([\n",
    "            T.Resize((self.config.IMG_SIZE, self.config.IMG_SIZE)),\n",
    "            T.RandomHorizontalFlip(0.5),\n",
    "            T.RandomVerticalFlip(0.5),\n",
    "            T.RandomRotation(45),\n",
    "            T.ToTensor(),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        val_transform = T.Compose([\n",
    "            T.Resize((self.config.IMG_SIZE, self.config.IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        return train_transform, val_transform\n",
    "    \n",
    "    def create_dataloaders(self) -> Tuple[DataLoader, DataLoader, int, List[str]]:\n",
    "        \"\"\"\n",
    "        Erstellt DataLoader für Training und Validierung mit Stratified Split.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple mit (train_loader, val_loader, num_classes, class_names)\n",
    "        \"\"\"\n",
    "        train_transform, val_transform = self.get_transforms()\n",
    "        \n",
    "        # Vollständigen Dataset laden\n",
    "        full_dataset = ImageFolder(self.config.DATA_FOLDER, transform=None)\n",
    "        num_classes = len(full_dataset.classes)\n",
    "        class_names = full_dataset.classes\n",
    "        \n",
    "        # Labels für Stratified Split extrahieren\n",
    "        targets = [full_dataset[i][1] for i in range(len(full_dataset))]\n",
    "        indices = list(range(len(full_dataset)))\n",
    "        \n",
    "        # Stratified Split durchführen\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            indices,\n",
    "            test_size=1-self.config.TRAIN_SPLIT,\n",
    "            stratify=targets,\n",
    "            random_state=self.config.SEED\n",
    "        )\n",
    "        \n",
    "        # Separate Datasets mit entsprechenden Transformationen erstellen\n",
    "        train_dataset_transformed = ImageFolder(self.config.DATA_FOLDER, transform=train_transform)\n",
    "        val_dataset_transformed = ImageFolder(self.config.DATA_FOLDER, transform=val_transform)\n",
    "        \n",
    "        # Subsets erstellen\n",
    "        from torch.utils.data import Subset\n",
    "        train_subset = Subset(train_dataset_transformed, train_indices)\n",
    "        val_subset = Subset(val_dataset_transformed, val_indices)\n",
    "        \n",
    "        # DataLoader erstellen\n",
    "        num_workers = min(os.cpu_count(), 8)  # Begrenzt auf max 8 Worker\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_subset, \n",
    "            batch_size=self.config.BATCH_SIZE, \n",
    "            shuffle=True, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_subset, \n",
    "            batch_size=self.config.BATCH_SIZE, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f'[STRATIFIED] Split completed')\n",
    "        self.logger.info(f'Train Dataset: {len(train_subset)} images')\n",
    "        self.logger.info(f'Val Dataset: {len(val_subset)} images')\n",
    "        self.logger.info(f'Number of classes: {num_classes}')\n",
    "        \n",
    "        return train_loader, val_loader, num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0aa5a-fdfe-4b02-8871-730769e50ac6",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. MODEL MANAGER CLASS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ca23a2-0bb4-4166-83ad-c05fb4e57bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Verwaltet Model-bezogene Operationen.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, num_classes: int, device: torch.device):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def create_model(self) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Erstellt und konfiguriert das ResNet-18 Model.\n",
    "        \n",
    "        Returns:\n",
    "            Konfiguriertes PyTorch Model\n",
    "        \"\"\"\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        model.fc = nn.Linear(model.fc.in_features, self.num_classes)\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        self.logger.info(f\"Model created with {self.num_classes} classes\")\n",
    "        return model\n",
    "    \n",
    "    def create_optimizer(self, model: nn.Module) -> torch.optim.Optimizer:\n",
    "        \"\"\"\n",
    "        Erstellt den Optimizer.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch Model\n",
    "            \n",
    "        Returns:\n",
    "            Konfigurierter Optimizer\n",
    "        \"\"\"\n",
    "        return optim.Adam(model.parameters(), lr=self.config.LEARNING_RATE)\n",
    "    \n",
    "    def load_model(self, model: nn.Module, model_path: str) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Lädt ein gespeichertes Model.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch Model\n",
    "            model_path: Pfad zum Model\n",
    "            \n",
    "        Returns:\n",
    "            Model mit geladenen Gewichten\n",
    "        \"\"\"\n",
    "        full_path = os.path.join(self.config.MODEL_FOLDER, model_path)\n",
    "        \n",
    "        if os.path.exists(full_path):\n",
    "            if torch.cuda.is_available():\n",
    "                model.load_state_dict(torch.load(full_path, weights_only=True))\n",
    "            else:\n",
    "                model.load_state_dict(\n",
    "                    torch.load(full_path, weights_only=True, map_location=torch.device('cpu'))\n",
    "                )\n",
    "            model.to(self.device)\n",
    "            self.logger.info(f\"Model loaded from {full_path}\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Model file {full_path} not found\")\n",
    "            \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71823135-5e2a-437b-b54c-35939442a020",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. METRICS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f25073e9-b748-4c20-bd27-c96a0919fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Metrics:\n",
    "    \"\"\"Datenklasse für Trainingsmetriken.\"\"\"\n",
    "    loss: float\n",
    "    accuracy: float\n",
    "    f1: float\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"Loss: {self.loss:.4f}, Acc: {self.accuracy:.4f}, F1: {self.f1:.4f}\"\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Berechnet verschiedene Metriken.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(predictions: np.ndarray, labels: np.ndarray, \n",
    "                         total_loss: float, dataset_size: int) -> Metrics:\n",
    "        \"\"\"\n",
    "        Berechnet Metriken aus Predictions und Labels.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Predicted labels\n",
    "            labels: True labels\n",
    "            total_loss: Gesamtverlust\n",
    "            dataset_size: Größe des Datasets\n",
    "            \n",
    "        Returns:\n",
    "            Metrics object\n",
    "        \"\"\"\n",
    "        avg_loss = total_loss / dataset_size\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions, average='weighted')\n",
    "        \n",
    "        return Metrics(loss=avg_loss, accuracy=accuracy, f1=f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31079d-3655-422a-a81f-9b840a14ffe4",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. METRICS MANAGER\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f020c560-71ba-4c00-8202-321886b97a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsManager:\n",
    "    \"\"\"Verwaltet das Speichern und Laden von Metriken und Evaluationsergebnissen.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.results_folder = Path(config.SAMPLE_FOLDER) / \"evaluation_results\"\n",
    "        self.results_folder.mkdir(exist_ok=True)\n",
    "        \n",
    "    def save_evaluation_results(self, \n",
    "                              results: Dict, \n",
    "                              class_names: List[str],\n",
    "                              model_name: str = \"model\",\n",
    "                              dataset_type: str = \"validation\") -> str:\n",
    "        \"\"\"\n",
    "        Speichert Evaluationsergebnisse mit korrekter JSON-Serialisierung.\n",
    "        \n",
    "        Args:\n",
    "            results: Ergebnisse von trainer.evaluate()\n",
    "            class_names: Namen der Klassen\n",
    "            model_name: Name des Models\n",
    "            dataset_type: Art des Datasets (train/validation/test)\n",
    "            \n",
    "        Returns:\n",
    "            Pfad zur gespeicherten JSON-Datei\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{model_name}_{dataset_type}_results_{timestamp}\"\n",
    "        \n",
    "        # Daten für JSON serialisierbar machen mit convert_numpy_types\n",
    "        save_data = {\n",
    "            'model_name': model_name,\n",
    "            'dataset_type': dataset_type,\n",
    "            'timestamp': timestamp,\n",
    "            'class_names': class_names,\n",
    "            'metrics': {\n",
    "                'loss': convert_numpy_types(results['loss']),\n",
    "                'accuracy': convert_numpy_types(results['accuracy']),\n",
    "                'f1': convert_numpy_types(results['f1'])\n",
    "            },\n",
    "            'predictions': convert_numpy_types(results['predictions']),\n",
    "            'labels': convert_numpy_types(results['labels']),\n",
    "            'num_classes': len(class_names),\n",
    "            'dataset_size': len(results['labels'])\n",
    "        }\n",
    "        \n",
    "        # Als JSON speichern mit NumpyEncoder als Backup\n",
    "        json_path = self.results_folder / f\"{filename}.json\"\n",
    "        try:\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(save_data, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)\n",
    "        except TypeError as e:\n",
    "            print(f\"⚠️  JSON serialization error: {e}\")\n",
    "            # Fallback: Speichere nur die konvertierten Daten\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        # Als Pickle für vollständige Python-Objekte (immer sicher)\n",
    "        pickle_path = self.results_folder / f\"{filename}.pkl\"\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "            \n",
    "        print(f\"[SAVED] Evaluation results saved:\")\n",
    "        print(f\"  JSON: {json_path}\")\n",
    "        print(f\"  Pickle: {pickle_path}\")\n",
    "        \n",
    "        return str(json_path)\n",
    "    \n",
    "    def load_evaluation_results(self, file_path: Union[str, Path]) -> Dict:\n",
    "        \"\"\"\n",
    "        Lädt gespeicherte Evaluationsergebnisse.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Pfad zur Datei (.json oder .pkl)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mit Evaluationsergebnissen\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        if file_path.suffix == '.json':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        elif file_path.suffix == '.pkl':\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        else:\n",
    "            raise ValueError(\"File must be .json or .pkl\")\n",
    "            \n",
    "        # Arrays zurück konvertieren\n",
    "        data['predictions'] = np.array(data['predictions'])\n",
    "        data['labels'] = np.array(data['labels'])\n",
    "        \n",
    "        print(f\"[LOADED] Evaluation results from {file_path}\")\n",
    "        print(f\"  Model: {data['model_name']}\")\n",
    "        print(f\"  Dataset: {data['dataset_type']}\") \n",
    "        print(f\"  Timestamp: {data['timestamp']}\")\n",
    "        print(f\"  Accuracy: {data['metrics']['accuracy']:.4f}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def list_saved_results(self) -> List[Dict]:\n",
    "        \"\"\"Listet alle gespeicherten Evaluationsergebnisse auf.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for json_file in self.results_folder.glob(\"*.json\"):\n",
    "            try:\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    results.append({\n",
    "                        'file': str(json_file),\n",
    "                        'model_name': data['model_name'],\n",
    "                        'dataset_type': data['dataset_type'],\n",
    "                        'timestamp': data['timestamp'],\n",
    "                        'accuracy': data['metrics']['accuracy'],\n",
    "                        'f1': data['metrics']['f1']\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {json_file}: {e}\")\n",
    "                \n",
    "        # Nach Timestamp sortieren\n",
    "        results.sort(key=lambda x: x['timestamp'], reverse=True)\n",
    "        \n",
    "        print(f\"\\n[AVAILABLE RESULTS] Found {len(results)} saved evaluation results:\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"{i+1:2d}. {result['model_name']} ({result['dataset_type']}) - \"\n",
    "                  f\"Acc: {result['accuracy']:.4f} - {result['timestamp']}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144196f-6877-4784-8ed1-834dfedb262b",
   "metadata": {},
   "source": [
    "---\n",
    "### 8. TRAINER CLASS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d97fd6-7a2d-47d4-bac2-fbbe37699ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Hauptklasse für das Training.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                 device: torch.device):\n",
    "        \"\"\"\n",
    "        Initialisiert den Trainer.\n",
    "        \n",
    "        Args:\n",
    "            config: Konfigurationsobjekt\n",
    "            model: PyTorch Model\n",
    "            optimizer: Optimizer\n",
    "            device: Device (CPU/GPU)\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Best values tracking\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_accuracy = 0.0\n",
    "        self.best_f1 = 0.0\n",
    "        \n",
    "        # History\n",
    "        self.history = {\n",
    "            \"train\": {\"loss\": [], \"accuracy\": [], \"f1\": []}, \n",
    "            \"val\": {\"loss\": [], \"accuracy\": [], \"f1\": []}\n",
    "        }\n",
    "        \n",
    "        # Scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=self.config.GAMMA, patience=3\n",
    "        )\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"Evaluiert das Model.\"\"\"\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            outputs = self.model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss, \n",
    "            'accuracy': accuracy, \n",
    "            'f1': f1,\n",
    "            'predictions': all_preds,\n",
    "            'labels': all_labels\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trainiert eine Epoche.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader für Training\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mit Trainingsergebnissen (konsistent mit evaluate())\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "        for images, labels in loop:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Predictions für Metriken\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            batch_acc = (predicted == labels).float().mean().item()\n",
    "            loop.set_postfix(loss=loss.item(), accuracy=batch_acc)\n",
    "        \n",
    "        # Metriken berechnen und als Dictionary zurückgeben\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    def save_best_models(self, val_metrics, epoch):\n",
    "        \"\"\"\n",
    "        Speichert beste Models basierend auf verschiedenen Metriken.\n",
    "        \"\"\"\n",
    "        updated = False\n",
    "        \n",
    "        # Best Loss Model\n",
    "        if val_metrics['loss'] < self.best_loss:\n",
    "            self.best_loss = val_metrics['loss']\n",
    "            loss_path = os.path.join(self.config.MODEL_FOLDER, 'best_loss_model.pth')\n",
    "            torch.save(self.model.state_dict(), loss_path)\n",
    "            print(f\"[BEST LOSS] Model saved at epoch {epoch + 1} (Loss: {val_metrics['loss']:.4f})\")\n",
    "            updated = True\n",
    "        \n",
    "        # Best Accuracy Model\n",
    "        if val_metrics['accuracy'] > self.best_accuracy:\n",
    "            self.best_accuracy = val_metrics['accuracy']\n",
    "            acc_path = os.path.join(self.config.MODEL_FOLDER, 'best_acc_model.pth')\n",
    "            torch.save(self.model.state_dict(), acc_path)\n",
    "            print(f\"[BEST ACC] Model saved at epoch {epoch + 1} (Acc: {val_metrics['accuracy']:.4f})\")\n",
    "            updated = True\n",
    "            \n",
    "        # Best F1 Model\n",
    "        if val_metrics['f1'] > self.best_f1:\n",
    "            self.best_f1 = val_metrics['f1']\n",
    "            f1_path = os.path.join(self.config.MODEL_FOLDER, 'best_f1_model.pth')\n",
    "            torch.save(self.model.state_dict(), f1_path)\n",
    "            print(f\"[BEST F1] Model saved at epoch {epoch + 1} (F1: {val_metrics['f1']:.4f})\")\n",
    "            updated = True\n",
    "            \n",
    "        return updated\n",
    "    \n",
    "    def _save_model(self, filename: str) -> None:\n",
    "        \"\"\"Speichert Model.\"\"\"\n",
    "        path = os.path.join(self.config.MODEL_FOLDER, filename)\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "    \n",
    "    def fit(self, train_loader, val_loader):\n",
    "        \"\"\"Haupttraining Loop.\"\"\"\n",
    "        early_stop_counter = 0\n",
    "        \n",
    "        for epoch in range(self.config.EPOCHS):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            print(f\"{'='*20} Epoch {epoch + 1}/{self.config.EPOCHS} {'='*20}\")\n",
    "            \n",
    "            # Training\n",
    "            train_metrics = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_metrics = self.evaluate(val_loader)\n",
    "            \n",
    "            # Update history - Dictionary Zugriff\n",
    "            self.history['train']['loss'].append(train_metrics['loss'])\n",
    "            self.history['train']['accuracy'].append(train_metrics['accuracy'])\n",
    "            self.history['train']['f1'].append(train_metrics['f1'])\n",
    "            \n",
    "            self.history['val']['loss'].append(val_metrics['loss'])\n",
    "            self.history['val']['accuracy'].append(val_metrics['accuracy'])\n",
    "            self.history['val']['f1'].append(val_metrics['f1'])\n",
    "            \n",
    "            # Save best models\n",
    "            if self.save_best_models(val_metrics, epoch):\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                print(f\"[NO IMPROVEMENT] Early stop counter: {early_stop_counter}/{self.config.PATIENCE}\")\n",
    "            \n",
    "            # Save epoch model\n",
    "            # self.save_epoch_model(epoch)\n",
    "            \n",
    "            # Epoch time logging\n",
    "            epoch_time = time.time() - start_time\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Logging - Dictionary Zugriff\n",
    "            print(f\"Train - Loss: {train_metrics['loss']:.4f}, \"\n",
    "                  f\"Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}\")\n",
    "            print(f\"Val   - Loss: {val_metrics['loss']:.4f}, \"\n",
    "                  f\"Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "            print(f\"LR: {current_lr:.6f}, Time: {epoch_time:.2f}s\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if early_stop_counter >= self.config.PATIENCE:\n",
    "                print(\"[EARLY STOP] Training stopped due to no improvement!\")\n",
    "                break\n",
    "            \n",
    "            # Learning rate scheduling - Dictionary Zugriff\n",
    "            self.scheduler.step(val_metrics['f1'])\n",
    "            \n",
    "            print()  # Leere Zeile für bessere Lesbarkeit\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea28ac6-fc7f-4b6e-bddb-60878a3c9d55",
   "metadata": {},
   "source": [
    "---\n",
    "### 8. VISUALIZATION CLASS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3501d8-f057-476d-94e3-b6feea9e0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    \"\"\"Handhabt alle Visualisierungen.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def plot_history(self, history, save: bool = True):\n",
    "        \"\"\"\n",
    "        Plottet Training History.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history dictionary\n",
    "            save: Ob Plot gespeichert werden soll\n",
    "        \"\"\"\n",
    "        train_history = history['train']\n",
    "        val_history = history['val']\n",
    "        epochs = range(1, len(train_history['loss']) + 1)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Loss Plot\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(epochs, train_history['loss'], 'bo-', label='Train Loss', linewidth=2)\n",
    "        plt.plot(epochs, val_history['loss'], 'ro-', label='Val Loss', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy Plot\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(epochs, train_history['accuracy'], 'bo-', label='Train Accuracy', linewidth=2)\n",
    "        plt.plot(epochs, val_history['accuracy'], 'ro-', label='Val Accuracy', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # F1 Score Plot\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(epochs, train_history['f1'], 'bo-', label='Train F1', linewidth=2)\n",
    "        plt.plot(epochs, val_history['f1'], 'ro-', label='Val F1', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('Training and Validation F1 Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            plot_path = os.path.join(self.config.SAMPLE_FOLDER, 'training_history.png')\n",
    "            history_path = os.path.join(self.config.SAMPLE_FOLDER, 'history.json')\n",
    "            \n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            with open(history_path, 'w') as f:\n",
    "                json.dump(history, f, indent=4)\n",
    "            \n",
    "            print(f\"[SAVED] Training plots saved to: {plot_path}\")\n",
    "            print(f\"[SAVED] History saved to: {history_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_true, y_pred, class_names, \n",
    "                            title=\"Confusion Matrix\", save: bool = True, \n",
    "                            save_name: Optional[str] = None, show: bool = True):\n",
    "        \"\"\"\n",
    "        Plottet eine detaillierte Confusion Matrix.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "            class_names: Namen der Klassen\n",
    "            title: Titel des Plots\n",
    "            save: Ob Plot gespeichert werden soll\n",
    "            save_name: Benutzerdefinierter Dateiname\n",
    "            show: Ob Plot angezeigt werden soll\n",
    "            \n",
    "        Returns:\n",
    "            Pfad zur gespeicherten Datei (falls save=True)\n",
    "        \"\"\"\n",
    "        # Confusion Matrix berechnen\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Normalized Confusion Matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Zwei Subplots: Absolute und Normalized\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Plot 1: Absolute Confusion Matrix\n",
    "        disp1 = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "        disp1.plot(ax=ax1, cmap='Blues', xticks_rotation=45)\n",
    "        ax1.set_title(f'{title} - Absolute Values')\n",
    "        ax1.grid(False)\n",
    "        \n",
    "        # Plot 2: Normalized Confusion Matrix\n",
    "        disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_names)\n",
    "        disp2.plot(ax=ax2, cmap='Blues', xticks_rotation=45, values_format='.2f')\n",
    "        ax2.set_title(f'{title} - Normalized')\n",
    "        ax2.grid(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        saved_path = None  # Default return value\n",
    "        if save:\n",
    "            if save_name is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                save_name = f\"confusion_matrix_{timestamp}.png\"\n",
    "            \n",
    "            saved_path = os.path.join(self.config.SAMPLE_FOLDER, save_name)\n",
    "            plt.savefig(saved_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"[SAVED] Confusion Matrix saved to: {saved_path}\")\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        \n",
    "        # Klassifikationsreport ausgeben\n",
    "        self.print_classification_report(y_true, y_pred, class_names)\n",
    "        \n",
    "        return saved_path  # Gibt None zurück falls nicht gespeichert\n",
    "    \n",
    "    def print_classification_report(self, y_true, y_pred, class_names):\n",
    "        \"\"\"\n",
    "        Druckt detaillierten Klassifikationsreport.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels  \n",
    "            class_names: Namen der Klassen\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import classification_report\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        report = classification_report(\n",
    "            y_true, y_pred, \n",
    "            target_names=class_names,\n",
    "            digits=4,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(report)\n",
    "        \n",
    "        # Zusätzliche Metriken\n",
    "        overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        print(f\"\\nOVERALL METRICS:\")\n",
    "        print(f\"  Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"  Macro F1 Score:   {macro_f1:.4f}\")\n",
    "        print(f\"  Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "        \n",
    "        # Per-class Accuracy\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "        \n",
    "        print(f\"\\nPER-CLASS ACCURACY:\")\n",
    "        for i, (class_name, acc) in enumerate(zip(class_names, per_class_acc)):\n",
    "            print(f\"  {class_name[:20]:<20}: {acc:.4f}\")\n",
    "    \n",
    "    def plot_model_performance_summary(self, history, y_true, y_pred, class_names):\n",
    "        \"\"\"\n",
    "        Erstellt ein umfassendes Performance Dashboard.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "            class_names: Namen der Klassen\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Training History (oben)\n",
    "        train_history = history['train']\n",
    "        val_history = history['val']\n",
    "        epochs = range(1, len(train_history['loss']) + 1)\n",
    "        \n",
    "        # Loss Plot\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.plot(epochs, train_history['loss'], 'b-', label='Train', linewidth=2)\n",
    "        plt.plot(epochs, val_history['loss'], 'r-', label='Validation', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy Plot\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(epochs, train_history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "        plt.plot(epochs, val_history['accuracy'], 'r-', label='Validation', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # F1 Score Plot\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.plot(epochs, train_history['f1'], 'b-', label='Train', linewidth=2)\n",
    "        plt.plot(epochs, val_history['f1'], 'r-', label='Validation', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('Training F1 Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confusion Matrix (unten links)\n",
    "        plt.subplot(2, 3, 4)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        im = plt.imshow(cm_normalized, interpolation='nearest', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (Normalized)')\n",
    "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Klassen-Labels (verkürzt für bessere Lesbarkeit)\n",
    "        short_labels = [name[:8] for name in class_names]\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, short_labels, rotation=45)\n",
    "        plt.yticks(tick_marks, short_labels)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        \n",
    "        # Per-Class Performance (unten mitte)\n",
    "        plt.subplot(2, 3, 5)\n",
    "        per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "        \n",
    "        bars = plt.bar(range(len(class_names)), per_class_acc, color='skyblue', alpha=0.7)\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Per-Class Accuracy')\n",
    "        plt.xticks(range(len(class_names)), short_labels, rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight best and worst performing classes\n",
    "        best_idx = np.argmax(per_class_acc)\n",
    "        worst_idx = np.argmin(per_class_acc)\n",
    "        bars[best_idx].set_color('green')\n",
    "        bars[worst_idx].set_color('red')\n",
    "        \n",
    "        # Summary Statistics (unten rechts)\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Calculate summary stats\n",
    "        overall_acc = accuracy_score(y_true, y_pred)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        best_class = class_names[best_idx]\n",
    "        worst_class = class_names[worst_idx]\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "        FINAL PERFORMANCE SUMMARY\n",
    "        \n",
    "        Overall Accuracy: {overall_acc:.3f}\n",
    "        Macro F1 Score: {macro_f1:.3f}\n",
    "        Weighted F1 Score: {weighted_f1:.3f}\n",
    "        \n",
    "        Best Performing Class:\n",
    "        {best_class} ({per_class_acc[best_idx]:.3f})\n",
    "        \n",
    "        Worst Performing Class:\n",
    "        {worst_class} ({per_class_acc[worst_idx]:.3f})\n",
    "        \n",
    "        Total Training Epochs: {len(epochs)}\n",
    "        Final Train Loss: {train_history['loss'][-1]:.4f}\n",
    "        Final Val Loss: {val_history['loss'][-1]:.4f}\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.text(0.1, 0.9, summary_text, transform=plt.gca().transAxes, \n",
    "                fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the dashboard\n",
    "        dashboard_path = os.path.join(self.config.SAMPLE_FOLDER, 'performance_dashboard.png')\n",
    "        plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"[SAVED] Performance Dashboard saved to: {dashboard_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def create_confusion_matrix_from_data(self, evaluation_data: Dict, \n",
    "                                        title: Optional[str] = None,\n",
    "                                        save: bool = True, show: bool = True, \n",
    "                                        save_name: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Erstellt Confusion Matrix aus geladenen Evaluationsdaten.\n",
    "        \n",
    "        Args:\n",
    "            evaluation_data: Geladene Evaluationsdaten\n",
    "            title: Titel für den Plot\n",
    "            save: Ob Plot gespeichert werden soll\n",
    "            show: Ob Plot angezeigt werden soll\n",
    "            save_name: Benutzerdefinierter Dateiname\n",
    "            \n",
    "        Returns:\n",
    "            Pfad zur gespeicherten Datei (falls save=True)\n",
    "        \"\"\"\n",
    "        y_true = evaluation_data['labels']\n",
    "        y_pred = evaluation_data['predictions']\n",
    "        class_names = evaluation_data['class_names']\n",
    "        \n",
    "        if title is None:\n",
    "            title = f\"{evaluation_data['model_name']} - {evaluation_data['dataset_type'].title()}\"\n",
    "        \n",
    "        # Verwende die erweiterte plot_confusion_matrix Methode\n",
    "        return self.plot_confusion_matrix(y_true, y_pred, class_names, title, save, save_name, show)\n",
    "    \n",
    "    def create_performance_comparison(self, evaluation_results: List[Dict],\n",
    "                                    save: bool = True, show: bool = True,\n",
    "                                    save_name: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Erstellt Vergleichsplot für mehrere Evaluationsergebnisse.\n",
    "        \n",
    "        Args:\n",
    "            evaluation_results: Liste von Evaluationsdaten\n",
    "            save: Ob Plot gespeichert werden soll\n",
    "            show: Ob Plot angezeigt werden soll\n",
    "            save_name: Benutzerdefinierter Dateiname\n",
    "            \n",
    "        Returns:\n",
    "            Pfad zur gespeicherten Datei (falls save=True)\n",
    "        \"\"\"\n",
    "        if len(evaluation_results) < 2:\n",
    "            print(\"Need at least 2 evaluation results for comparison\")\n",
    "            return None\n",
    "            \n",
    "        # Daten extrahieren\n",
    "        models = []\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        losses = []\n",
    "        \n",
    "        for result in evaluation_results:\n",
    "            models.append(f\"{result['model_name']}\\n({result['dataset_type']})\")\n",
    "            accuracies.append(result['metrics']['accuracy'])\n",
    "            f1_scores.append(result['metrics']['f1'])\n",
    "            losses.append(result['metrics']['loss'])\n",
    "        \n",
    "        # Plot erstellen\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Accuracy Comparison\n",
    "        bars1 = ax1.bar(models, accuracies, color='skyblue', alpha=0.7)\n",
    "        ax1.set_title('Accuracy Comparison')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # F1 Score Comparison\n",
    "        bars2 = ax2.bar(models, f1_scores, color='lightgreen', alpha=0.7)\n",
    "        ax2.set_title('F1 Score Comparison')\n",
    "        ax2.set_ylabel('F1 Score')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss Comparison\n",
    "        bars3 = ax3.bar(models, losses, color='lightcoral', alpha=0.7)\n",
    "        ax3.set_title('Loss Comparison')\n",
    "        ax3.set_ylabel('Loss')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Werte auf Balken anzeigen\n",
    "        for bars, values in [(bars1, accuracies), (bars2, f1_scores), (bars3, losses)]:\n",
    "            for bar, value in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                bars[0].axes.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                                f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        saved_path = None\n",
    "        if save:\n",
    "            if save_name is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                save_name = f\"model_comparison_{timestamp}.png\"\n",
    "                \n",
    "            saved_path = os.path.join(self.config.SAMPLE_FOLDER, save_name)\n",
    "            plt.savefig(saved_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"[SAVED] Model Comparison: {saved_path}\")\n",
    "            \n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "            \n",
    "        return saved_path\n",
    "    \n",
    "    def create_training_history_plot(self, history_data: Dict,\n",
    "                                   title: Optional[str] = None,\n",
    "                                   save: bool = True, show: bool = True,\n",
    "                                   save_name: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Erstellt Training History Plot aus gespeicherten Daten.\n",
    "        \n",
    "        Args:\n",
    "            history_data: Training History Dictionary\n",
    "            title: Titel für den Plot\n",
    "            save: Ob Plot gespeichert werden soll\n",
    "            show: Ob Plot angezeigt werden soll\n",
    "            save_name: Benutzerdefinierter Dateiname\n",
    "            \n",
    "        Returns:\n",
    "            Pfad zur gespeicherten Datei (falls save=True)\n",
    "        \"\"\"\n",
    "        train_history = history_data['train']\n",
    "        val_history = history_data['val']\n",
    "        epochs = range(1, len(train_history['loss']) + 1)\n",
    "        \n",
    "        if title is None:\n",
    "            title = \"Training History\"\n",
    "            \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Loss Plot\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(epochs, train_history['loss'], 'bo-', label='Train Loss', linewidth=2)\n",
    "        plt.plot(epochs, val_history['loss'], 'ro-', label='Val Loss', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'{title} - Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy Plot\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(epochs, train_history['accuracy'], 'bo-', label='Train Accuracy', linewidth=2)\n",
    "        plt.plot(epochs, val_history['accuracy'], 'ro-', label='Val Accuracy', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'{title} - Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # F1 Score Plot\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(epochs, train_history['f1'], 'bo-', label='Train F1', linewidth=2)\n",
    "        plt.plot(epochs, val_history['f1'], 'ro-', label='Val F1', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'{title} - F1 Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        saved_path = None\n",
    "        if save:\n",
    "            if save_name is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                save_name = f\"training_history_{timestamp}.png\"\n",
    "            \n",
    "            saved_path = os.path.join(self.config.SAMPLE_FOLDER, save_name)\n",
    "            plt.savefig(saved_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"[SAVED] Training History: {saved_path}\")\n",
    "            \n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "            \n",
    "        return saved_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2018b0-63c0-47b2-a16a-35732c28e2d9",
   "metadata": {},
   "source": [
    "---\n",
    "### 9. EVALUATION WORKFLOW KLASSE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2365fa58-c054-4aee-af2f-61bcf6be1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationWorkflow:\n",
    "    \"\"\"Vereinfacht den Workflow für Evaluation und Visualisierung.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.metrics_manager = MetricsManager(config)\n",
    "        self.visualizer = Visualizer(config)\n",
    "        \n",
    "    def save_training_results(self, \n",
    "                            classifier,\n",
    "                            model_name: str = \"resnet18\",\n",
    "                            save_train: bool = True,\n",
    "                            save_val: bool = True) -> Tuple[Optional[str], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Speichert Training- und Validierungsergebnisse nach dem Training.\n",
    "        \"\"\"\n",
    "        train_path = None\n",
    "        val_path = None\n",
    "        \n",
    "        if save_train:\n",
    "            train_results = classifier.trainer.evaluate(classifier.train_loader)\n",
    "            train_path = self.metrics_manager.save_evaluation_results(\n",
    "                train_results, classifier.class_names, model_name, \"training\"\n",
    "            )\n",
    "            \n",
    "        if save_val:\n",
    "            val_results = classifier.trainer.evaluate(classifier.val_loader)\n",
    "            val_path = self.metrics_manager.save_evaluation_results(\n",
    "                val_results, classifier.class_names, model_name, \"validation\"\n",
    "            )\n",
    "            \n",
    "        return train_path, val_path\n",
    "    \n",
    "    def load_and_visualize(self, \n",
    "                          result_file: Union[str, Path],\n",
    "                          create_confusion_matrix: bool = True,\n",
    "                          save_plots: bool = True,\n",
    "                          show_plots: bool = True) -> Dict[str, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Lädt Evaluationsergebnisse und erstellt Visualisierungen.\n",
    "        \"\"\"\n",
    "        # Daten laden\n",
    "        evaluation_data = self.metrics_manager.load_evaluation_results(result_file)\n",
    "        \n",
    "        saved_plots = {}\n",
    "        \n",
    "        if create_confusion_matrix:\n",
    "            cm_path = self.visualizer.create_confusion_matrix_from_data(\n",
    "                evaluation_data, save=save_plots, show=show_plots\n",
    "            )\n",
    "            saved_plots['confusion_matrix'] = cm_path\n",
    "            \n",
    "        return saved_plots\n",
    "    \n",
    "    def compare_models(self, \n",
    "                      result_files: List[Union[str, Path]],\n",
    "                      save_comparison: bool = True,\n",
    "                      show_comparison: bool = True) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Vergleicht mehrere Models.\n",
    "        \"\"\"\n",
    "        evaluation_results = []\n",
    "        \n",
    "        for file_path in result_files:\n",
    "            data = self.metrics_manager.load_evaluation_results(file_path)\n",
    "            evaluation_results.append(data)\n",
    "            \n",
    "        return self.visualizer.create_performance_comparison(\n",
    "            evaluation_results, save=save_comparison, show=show_comparison\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b913ba-5b01-4650-990e-c9a19f484fe8",
   "metadata": {},
   "source": [
    "---\n",
    "### 10. STANDALONE MODEL EVALUATOR KLASSE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e14e50-133a-4ea0-9cf5-93d9cea6d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandaloneModelEvaluator:\n",
    "    \"\"\"\n",
    "    Lädt gespeicherte Models und evaluiert sie unabhängig vom Training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = get_device()\n",
    "        \n",
    "        # Data Manager für DataLoader\n",
    "        self.data_manager = DataManager(config)\n",
    "        (self.train_loader, self.val_loader, \n",
    "         self.num_classes, self.class_names) = self.data_manager.create_dataloaders()\n",
    "        \n",
    "        # Model Manager\n",
    "        self.model_manager = ModelManager(config, self.num_classes, self.device)\n",
    "        \n",
    "        # Evaluation Workflow\n",
    "        self.eval_workflow = EvaluationWorkflow(config)\n",
    "        \n",
    "    def evaluate_saved_model(self, \n",
    "                           model_path: str,\n",
    "                           model_name: str = \"loaded_model\",\n",
    "                           visualize: bool = True,\n",
    "                           save_results: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Lädt ein gespeichertes Model und evaluiert es vollständig.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Pfad zum Model (.pth Datei)\n",
    "            model_name: Name für die Speicherung\n",
    "            visualize: Ob Visualisierungen erstellt werden sollen\n",
    "            save_results: Ob Ergebnisse gespeichert werden sollen\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mit Evaluationsergebnissen\n",
    "        \"\"\"\n",
    "        print(f\"[EVALUATING] Loading model from {model_path}\")\n",
    "        \n",
    "        # Model erstellen und laden\n",
    "        model = self.model_manager.create_model()\n",
    "        model = self.model_manager.load_model(model, model_path)\n",
    "        \n",
    "        # Trainer für Evaluation erstellen\n",
    "        optimizer = self.model_manager.create_optimizer(model)  # Dummy optimizer\n",
    "        trainer = Trainer(self.config, model, optimizer, self.device)\n",
    "        \n",
    "        # Validation Set evaluieren\n",
    "        print(\"[EVALUATING] Running evaluation on validation set...\")\n",
    "        val_results = trainer.evaluate(self.val_loader)\n",
    "        \n",
    "        # Optional: Training Set evaluieren\n",
    "        print(\"[EVALUATING] Running evaluation on training set...\")\n",
    "        train_results = trainer.evaluate(self.train_loader)\n",
    "        \n",
    "        results = {\n",
    "            'validation': val_results,\n",
    "            'training': train_results,\n",
    "            'class_names': self.class_names,\n",
    "            'model_name': model_name\n",
    "        }\n",
    "        \n",
    "        if save_results:\n",
    "            print(\"[SAVING] Saving evaluation results...\")\n",
    "            val_path = self.eval_workflow.metrics_manager.save_evaluation_results(\n",
    "                val_results, self.class_names, model_name, \"validation\"\n",
    "            )\n",
    "            train_path = self.eval_workflow.metrics_manager.save_evaluation_results(\n",
    "                train_results, self.class_names, model_name, \"training\"\n",
    "            )\n",
    "            results['saved_files'] = {'validation': val_path, 'training': train_path}\n",
    "        \n",
    "        if visualize:\n",
    "            print(\"[VISUALIZING] Creating visualizations...\")\n",
    "            # Confusion Matrix für Validation\n",
    "            self.eval_workflow.visualizer.plot_confusion_matrix(\n",
    "                val_results['labels'], \n",
    "                val_results['predictions'], \n",
    "                self.class_names,\n",
    "                title=f\"{model_name} - Validation Results\",\n",
    "                save=True\n",
    "            )\n",
    "            \n",
    "            # Optional: Confusion Matrix für Training\n",
    "            self.eval_workflow.visualizer.plot_confusion_matrix(\n",
    "                train_results['labels'], \n",
    "                train_results['predictions'], \n",
    "                self.class_names,\n",
    "                title=f\"{model_name} - Training Results\", \n",
    "                save=True\n",
    "            )\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e818a-808c-439b-9d08-0962f2bc3d36",
   "metadata": {},
   "source": [
    "---\n",
    "### 11. PREDICTOR CLASS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e5f83a4-0689-4106-9a1a-810bdec0d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \"\"\"Handhabt Model Predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, val_transform: transforms.Compose,\n",
    "                 class_names: List[str], device: torch.device, config: Config):\n",
    "        self.model = model\n",
    "        self.val_transform = val_transform\n",
    "        self.class_names = class_names\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "    \n",
    "    def predict_image(self, source: Union[str, np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Sagt Klasse für ein einzelnes Bild vorher.\n",
    "        \n",
    "        Args:\n",
    "            source: Dateipfad oder Numpy Array\n",
    "            \n",
    "        Returns:\n",
    "            Vorhergesagte Klasse\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Bild laden\n",
    "        if isinstance(source, str):\n",
    "            if not os.path.exists(source):\n",
    "                raise FileNotFoundError(f\"{source} does not exist\")\n",
    "            image = Image.open(source).convert('RGB')\n",
    "        elif isinstance(source, np.ndarray):\n",
    "            if source.ndim == 2:\n",
    "                source = np.stack([source] * 3, axis=-1)\n",
    "            elif source.ndim == 3 and source.shape[2] == 1:\n",
    "                source = np.repeat(source, 3, axis=2)\n",
    "            image = Image.fromarray(source.astype('uint8')).convert('RGB')\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a file path (str) or image array (np.ndarray)\")\n",
    "        \n",
    "        # Transform und Prediction\n",
    "        input_tensor = self.val_transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            confidence, predicted = torch.max(probs, 1)\n",
    "        \n",
    "        if confidence.item() < self.config.PREDICTION_THRESHOLD:\n",
    "            return self.config.UNKNOWN_LABEL\n",
    "        else:\n",
    "            return self.class_names[predicted.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b9dda-4402-4409-9659-cd6fcdc9241e",
   "metadata": {},
   "source": [
    "---\n",
    "### 12. UTILITY FUNKTIONEN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2c565b2-a573-4b6f-888b-2482e76c114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_save_results(classifier, model_name=\"model\"):\n",
    "    \"\"\"Schnelle Funktion zum Speichern von Ergebnissen nach dem Training.\"\"\"\n",
    "    eval_workflow = EvaluationWorkflow(classifier.config)\n",
    "    return eval_workflow.save_training_results(classifier, model_name)\n",
    "\n",
    "def quick_visualize_latest(config):\n",
    "    \"\"\"Schnelle Funktion zur Visualisierung der neuesten Ergebnisse.\"\"\"\n",
    "    eval_workflow = EvaluationWorkflow(config)\n",
    "    \n",
    "    available = eval_workflow.metrics_manager.list_saved_results()\n",
    "    if available:\n",
    "        return eval_workflow.load_and_visualize(available[0]['file'])\n",
    "    else:\n",
    "        print(\"No saved results found!\")\n",
    "        return None\n",
    "\n",
    "def quick_compare_all(config):\n",
    "    \"\"\"Schnelle Funktion zum Vergleich aller verfügbaren Models.\"\"\"\n",
    "    eval_workflow = EvaluationWorkflow(config)\n",
    "    \n",
    "    available = eval_workflow.metrics_manager.list_saved_results()\n",
    "    if len(available) >= 2:\n",
    "        files = [r['file'] for r in available[:5]]  # Max 5 Models\n",
    "        return eval_workflow.compare_models(files)\n",
    "    else:\n",
    "        print(\"Need at least 2 saved results for comparison!\")\n",
    "        return None\n",
    "\n",
    "def evaluate_model_from_file(model_path: str, config: Config = None, model_name: str = \"loaded_model\"):\n",
    "    \"\"\"\n",
    "    Standalone Funktion zur Evaluation eines gespeicherten Models.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Pfad zur .pth Model-Datei\n",
    "        config: Config object (wird erstellt falls None)\n",
    "        model_name: Name für die Speicherung\n",
    "        \n",
    "    Returns:\n",
    "        Evaluationsergebnisse\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = Config()\n",
    "    \n",
    "    evaluator = StandaloneModelEvaluator(config)\n",
    "    return evaluator.evaluate_saved_model(model_path, model_name)\n",
    "\n",
    "def show_confusion_matrix_for_loaded_model(model_path):\n",
    "    \"\"\"Confusion Matrix für bereits geladenes Model.\"\"\"\n",
    "    \n",
    "    config = Config()\n",
    "    classifier = StreetFoodClassifier(config)\n",
    "    \n",
    "    # Model laden statt trainieren\n",
    "    classifier.load_and_evaluate(model_path)\n",
    "    \n",
    "    # Evaluation und Confusion Matrix\n",
    "    val_results = classifier.trainer.evaluate(classifier.val_loader)\n",
    "    classifier.visualizer.plot_confusion_matrix(\n",
    "        y_true=val_results['labels'],\n",
    "        y_pred=val_results['predictions'],\n",
    "        class_names=classifier.class_names,\n",
    "        title=f\"Results for {model_path}\",\n",
    "        save=True\n",
    "    )\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom JSON Encoder für NumPy Datentypen.\"\"\"\n",
    "    \n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "def convert_numpy_types(obj: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Konvertiert NumPy Datentypen rekursiv zu Python Standard-Typen.\n",
    "    \n",
    "    Args:\n",
    "        obj: Objekt das konvertiert werden soll\n",
    "        \n",
    "    Returns:\n",
    "        Konvertiertes Objekt mit Python Standard-Typen\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439245a0-3dbc-4372-a081-0af30912839f",
   "metadata": {},
   "source": [
    "---\n",
    "### 13. MAIN APPLICATION CLASS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b43c3e5c-8f4a-4438-8e70-bc5278772139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFoodClassifier:\n",
    "    \"\"\"Hauptapplikation für Street Food Klassifikation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.logger = setup_logger()\n",
    "        \n",
    "        # Setup\n",
    "        seed_everything(config.SEED)\n",
    "        self.device = get_device()\n",
    "        \n",
    "        # Components\n",
    "        self.data_manager = DataManager(config) \n",
    "        self.visualizer = Visualizer(config)\n",
    "        \n",
    "        # Daten laden\n",
    "        (self.train_loader, self.val_loader, \n",
    "         self.num_classes, self.class_names) = self.data_manager.create_dataloaders()\n",
    "        \n",
    "        # Model Setup\n",
    "        self.model_manager = ModelManager(config, self.num_classes, self.device)\n",
    "        self.model = self.model_manager.create_model()\n",
    "        self.optimizer = self.model_manager.create_optimizer(self.model)\n",
    "        \n",
    "        # Trainer\n",
    "        self.trainer = Trainer(config, self.model, self.optimizer, self.device)\n",
    "        \n",
    "        # Predictor (wird nach Training/Loading erstellt)\n",
    "        self.predictor = None\n",
    "    \n",
    "    def train(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Startet das Training.\n",
    "        \n",
    "        Returns:\n",
    "            Training history\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting training...\")\n",
    "        history = self.trainer.fit(self.train_loader, self.val_loader)\n",
    "        self.logger.info(\"Training completed!\")\n",
    "        \n",
    "        # Predictor erstellen\n",
    "        _, val_transform = self.data_manager.get_transforms()\n",
    "        self.predictor = Predictor(\n",
    "            self.model, val_transform, self.class_names, self.device, self.config\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def load_and_evaluate(self, model_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Lädt Model und evaluiert es.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Pfad zum Model\n",
    "        \"\"\"\n",
    "        self.model = self.model_manager.load_model(self.model, model_path)\n",
    "        \n",
    "        # Predictor erstellen\n",
    "        _, val_transform = self.data_manager.get_transforms()\n",
    "        self.predictor = Predictor(\n",
    "            self.model, val_transform, self.class_names, self.device, self.config\n",
    "        )\n",
    "        \n",
    "        # Evaluation\n",
    "        test_dataset = ImageFolder(root=self.config.DATA_FOLDER, transform=val_transform)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=self.config.BATCH_SIZE, \n",
    "            shuffle=False, num_workers=4\n",
    "        )\n",
    "        \n",
    "        metrics = self.trainer.evaluate(test_loader)\n",
    "        self.logger.info(f\"Test Results: {metrics}\")\n",
    "    \n",
    "    def predict(self, image_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Sagt Klasse für ein Bild vorher.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Pfad zum Bild\n",
    "            \n",
    "        Returns:\n",
    "            Vorhergesagte Klasse\n",
    "        \"\"\"\n",
    "        if self.predictor is None:\n",
    "            raise RuntimeError(\"Model not trained or loaded yet!\")\n",
    "        \n",
    "        return self.predictor.predict_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c7725-fbe7-467b-91b8-46c5a2320d62",
   "metadata": {},
   "source": [
    "---\n",
    "### 14. BEISPIEL NUTZUNGEN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed87ee05-9245-4316-a49d-d4b7059dd2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_after_training(classifier):\n",
    "    \"\"\"Beispiel: Nach dem Training Ergebnisse speichern und visualisieren.\"\"\"\n",
    "    \n",
    "    # 1. Ergebnisse speichern\n",
    "    train_file, val_file = quick_save_results(classifier, \"my_resnet18_experiment\")\n",
    "    \n",
    "    # 2. Sofort visualisieren\n",
    "    val_results = classifier.trainer.evaluate(classifier.val_loader)\n",
    "    classifier.visualizer.plot_confusion_matrix(\n",
    "        val_results['labels'], \n",
    "        val_results['predictions'], \n",
    "        classifier.class_names,\n",
    "        title=\"Current Training Results\"\n",
    "    )\n",
    "\n",
    "def example_load_and_analyze():\n",
    "    \"\"\"Beispiel: Gespeicherte Ergebnisse laden und analysieren.\"\"\"\n",
    "    \n",
    "    config = Config()\n",
    "    \n",
    "    # 1. Verfügbare Ergebnisse anzeigen\n",
    "    eval_workflow = EvaluationWorkflow(config)\n",
    "    available_results = eval_workflow.metrics_manager.list_saved_results()\n",
    "    \n",
    "    # 2. Neuestes Ergebnis visualisieren\n",
    "    if available_results:\n",
    "        latest_result = available_results[0]['file']\n",
    "        eval_workflow.load_and_visualize(latest_result)\n",
    "\n",
    "def example_evaluate_saved_model():\n",
    "    \"\"\"Beispiel: Gespeichertes Model komplett neu evaluieren.\"\"\"\n",
    "    \n",
    "    config = Config()\n",
    "    \n",
    "    # Model von Datei laden und komplett evaluieren\n",
    "    results = evaluate_model_from_file(\n",
    "        model_path=\"best_f1_model.pth\",\n",
    "        config=config,\n",
    "        model_name=\"best_f1_loaded\"\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluation completed!\")\n",
    "    print(f\"Validation Accuracy: {results['validation']['accuracy']:.4f}\")\n",
    "    print(f\"Training Accuracy: {results['training']['accuracy']:.4f}\")\n",
    "\n",
    "def example_compare_models():\n",
    "    \"\"\"Beispiel: Mehrere Models vergleichen.\"\"\"\n",
    "    \n",
    "    config = Config()\n",
    "    eval_workflow = EvaluationWorkflow(config)\n",
    "    \n",
    "    # Alle verfügbaren Ergebnisse vergleichen\n",
    "    available = eval_workflow.metrics_manager.list_saved_results()\n",
    "    if len(available) >= 2:\n",
    "        files = [r['file'] for r in available[:3]]  # Top 3 vergleichen\n",
    "        eval_workflow.compare_models(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab037eb-ca9f-42b0-8dfb-2a1429dcb9e3",
   "metadata": {},
   "source": [
    "---\n",
    "### 15. Main\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f04dce-98da-4d9b-b453-1863cf779cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e94d0-9e05-4dd2-9ad6-86923a7659ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
